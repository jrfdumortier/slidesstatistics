---
title: "Multivariate Regression"
author: "Jerome Dumortier"
date: "`r format(Sys.time(),'%d %B %Y')`"
output: 
     beamer_presentation:
          theme: "Hannover"
classoption: "aspectratio=169"
linkcolor: MidnightBlue
---

```{r,echo=FALSE,warning=FALSE,message=FALSE,error=FALSE}
load("G:/My Drive/Teaching/Data Analysis for Public Affairs/GitHub/DataAnalysisPAData.RData")
```

## Lecture Overview
Extension of the bivariate model to multivariate regression:
        
- One dependent variable but multiple independent variables
        
Topics associated with multivariate regression models covered in this lecture:
        
- Dummy variables
- Natural logarithm
- Functional forms
- Interaction terms
        
## Introduction
Bivariate regression model (one independent and one dependent variable)
$$y = \beta_0 + \beta_1 \cdot x_1 + \epsilon$$
Multivariate linear regression model (multiple independent variables)
$$y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \dots + \beta_k \cdot x_k + \epsilon$$
Whether we consider a bivariate or multivariate model, the objective is to minimize the sum of squared errors, hence ordinary least square (OLS) model. The equation of a line can be determined using slope and intercept, i.e.:
$$E(y|x) = \beta_0 + \beta_1 \cdot x$$
A model with two independent variables (predictors) describes a plane.

## Assumptions
Key assumptions of the model are identical to the bivariate model:

- Zero mean of the error terms, i.e., $E(\epsilon | x_1, \dots, x_k) = 0$.
- No auto-regression and no serial correlation, i.e., $Cov(\epsilon_i,\epsilon_j)=0 \quad \vee \quad i \neq j$
- Homoscedasticity, $Var(\epsilon_i) = \sigma^2$
- Linearity, i.e., the model can be expressed as a linear relationship between $y$ and $x_1, \dots, x_k$.

With more than one independent variable, absence of perfect multicollinearity becomes important.
        
- For example, including wealth and income in a regression model may result in multicollinearity.
        
## Multivariate Regression Models
Purpose:
        
- Measuring the effect of one independent variable on the dependent variable
- Crucial issue: We have to control for everything else that could influence the dependent variable!
        
Why we have to control for everything:
        
- Weekly grocery bill as a function of years of education
        
Example: Crime rate depends on unemployment, population density, and high school dropout rate:
$$y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3$$
where $x_1$ = population density, $x_2$ = unemployment, and $x_3$ = high school dropout rate

## North Carolina County Data
\scriptsize 
```{r,echo=FALSE}
bhat = lm(violentcrimerate~popdense+unemployment+publicschoolenrollment,
          data = crime)
summary(bhat)
```
\normalsize

## Child Mortality: Setup
    
- We think that Child Mortality (CM), measured as the number of deaths per 1000 births, is related to the per-capita Gross National Product (PGNP) and the Female Literacy rate (FLFP).
- We might hypothesize that wealthier countries have lower child mortality rates, and countries with higher female literacy rates also have lower child mortality rates.
- Why? Maybe higher PGNP means more money for healthcare; Maybe better literacy rates mean more educated mothers that can take advantage of health related information.
- The data set also contains the total fertility rate (TFR) between 1980 and 1985, i.e., the average number of children born to a woman.

## Child Mortality: R Analysis
\footnotesize
```{r}
bhat=lm(CM~PGNP+FLFP,data=childmortality)
summary(bhat)
```
\normalsize

## Child Mortality: Interpretation
    
- It tells us the parameter estimates for PGNP and FLR. It also tells us if our estimate is statistically different from zero.
- As expected, both variables lower child mortality.
- As per capita GNP increases by one dollar, child mortality decreases by 0.565 units, holding the effects of female literacy constant.
- Since the units of CM are number of deaths per 1000 births, it is easier to say that for a \$1000 dollar increase in per capita GNP, the number of childhood deaths decreases by 5.6 per 1000. (since 0.00565 * 1000 = 5.6 units)
- Similarly, for a one unit (one percent) increase in the female literacy rate, the childhood mortality decreases by 2.23 deaths per 1000, holding the effects of PGNP constant.
    
## Child Mortality Data: Purpose of OLS I
Forecasting
        
- If a country as a per capita GNP of \$3,000 and a female literacy rate of 50, then the expected number of childhood deaths per 1000 births is 135.26.
- $\beta_1$ and $\beta_2$ as partial regression or partial slope coefficients:
                
     - $\beta_1$ quantifies the change in the expected mean of $Y$, i.e., $E(Y)$, per one unit increase/decrease in $x_1$ holding the value of $x_2$ constant.
     - Important: No need to know the value of $x_2$.

## Child Mortality Data: Purpose of OLS I        
Hypothesis testing
        
- We reject the null hypothesis that the effect of Per Capita GNP on childhood mortality is zero at the 1\% critical level, with a t-value of -2.82 and p-value of 0.0065.
- Similarly, we reject the null hypothesis of no relationship between female literacy rate and childhood mortality at the 1\% critical level. FLR has a t-value of -10.63 and a p-value of 0.0001.
        
## F-Test: Accident Data
\footnotesize
```{r}
bhat = lm(accidents~temperature+precipitation,data=accidents)
summary(bhat)
```
\normalsize

## F-Test: Accident Data
Hypothesis: $H_0$: $\beta_1=\beta_2=0$
$$\frac{R^2/(k-1)}{(1-R^2)/(n-k)} \sim F_{k-1,n-k}$$
Given the accident data: $n=30,k=3$
$$\frac{`r summary(bhat)$r.squared`/2}{(1-`r summary(bhat)$r.squared`)/(30-3)} = `r summary(bhat)$fstatistic[1]`$$ 

This is different and an extension from the previous versions of hypothesis testing. We are not conducting hypothesis tests on the individual parameters but on all slope coefficients simultaneously.

Comparing two $R^2$ values only if identical sample size and identical dependent variable.

# Dummy Variables
## Overview
Representation of a single qualitative characteristics coded as 0 or 1

- Examples: Religion, gender, nationality, all-wheel drive (AWD), hardwood floors
        
Used car examples where the $price$ depends on $miles$ and $AWD$ (i.e., a dummy variable):
$$price_i = \beta_0 + \beta_1 \cdot miles_i + \beta_2 \cdot AWD_i + \epsilon_i$$
with $AWD_i=1$ for an all-wheel drive car and $AWD_i=0$ for a car with no all-wheel drive. This regression can theoretically be separated into two single equations:

- RWD: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$
- AWD: $Y_i = (\beta_0 + \beta_2 )+ \beta_1 X_i + \epsilon_i$

Interpretation:
        
- Knowledge on how the dummy-variable was coded.
- If the coefficient of the dummy-variable "adds" (or "subtracts" if sign is negative) compared to the 0-group.

## Dummay Variables: Interpretation
\footnotesize
```{r}
bhat=lm(price ~ miles + allwheeldrive, data = bmw)
summary(bhat)
```
\normalsize

## Regressions Involving Natural Logarithms I
Consider the log-linear model:
$$y_i = \beta_0 \cdot x_i^{\beta_1} \cdot \epsilon_i$$
Taking the natural logarithm on both sides
$$\ln(y_i) = \ln(\beta_0) + \beta_1 \cdot \ln(x_i) + \epsilon_i$$
You can choose which variables you want to transform using the natural log. You can transform just the dependent variable and/or all (or just some) of the independent variables. However, the interpretation of the $\beta$ coefficients will change depending on your approach.

## Regressions Involving Natural Logarithms II

| Dep. Var. | Indep. Var. | Interpretation  |
|:---------:|:-----------:|:---------------------------------------------:|
| $y$       | $x$         | $\Delta y=\beta \cdot \Delta x$               |
| $y$       | $\ln(x)$    | $\Delta y=(\beta/100)\% \cdot \Delta x$       |
| $\ln(y)$  | $x$         | $\% \Delta y=(100 \cdot \beta)\cdot \Delta x$ |
| $\ln(y)$  | $\ln(x)$    | $\% \Delta y=\beta \% \cdot \Delta x$         |

For example, consider the following regression:
$$\ln(consumption)=\beta_0+\beta_1 \cdot \ln(income)$$
Assume $\beta_1=0.8$: A 1 percent increase in income results in a $0.8 \cdot 1\%=0.8\%$ increase in consumption.

## Dummy Variables and Natural Logarithm I
Consider the following model:
$$\ln(y)=\beta_0+\beta_1 \cdot X+\beta_2 \cdot D+\epsilon$$
In this case, $X$ is the continuous independent variable and $D$ is the dummy variable. $\beta_2$ is interpreted as follows:

- If $D$ switches from 0 to 1, the percent impact of $D$ on $Y$ is $100 \cdot (e^{\beta_2}-1)$.
- If $D$ switches from 1 to 0, the percent impact of $D$ on $Y$ is $100 \cdot (e^{\beta_2}-1)$.

## Dummy Variables and Natural Logarithm I
Interpretation when the Dependent Variable is $\ln(\cdot)$}
    Consider the following model:
        $$\ln(y) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot D + \epsilon$$
In this case, the interpretation of $\beta_1$ is $e^{\beta}-1$. So in the regression on the next slide, we have the coefficient for colonial which is 0.0538. Thus the feature "colonial" adds 5.53 percent to the value of the house.

## Dummy Variables and Natural Logarithm III
\footnotesize
```{r}
bhat=lm(log(price)~log(lotsize)+log(sqrft)+bdrms+
    colonial,data=housing1)
summary(bhat)
```
\normalsize

# Functional Form
## Examples of Functional Forms

- Relation between consumption and income: Change in consumption due to extra income may decrease with income.
- Relationship between income and education: Change in income due to more education may decrease with more education

Consider the following relationships between $y$ and $x$:

- $y = \beta_0 + \beta_1 x + \beta_2 x^2$
- $y = \beta_0 + \beta_1 x^{\beta_2}$

If a nonlinear relation can be expressed as a linear relation by redefining variables we can estimate that relation using ordinary least square.



## Functional Form
Relationship 1:

- Linear in the regression coefficients, i.e. it can be expressed as a linear relation between $y$ and independent variables $x_1$ and $x_2$: $x_1 = x$ and $x_2 = x^2$

Relationship 2:

- Taking the log of the dependent/independent variable can help making the model linear.

## Squared/Quadratic Terms
Consider a model with $x_2$ included as a squared term:
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_2^2$$
Change in $y$ due to a change in $x$:
$$\Delta \hat{y} \approx (\hat{\beta}_2 + 2 \cdot \hat{\beta}_2) \Delta x$$

## Squared/Quadratic Terms in R: `wage`
\scriptsize
```{r}
bhat=lm(income~educ+exper+I(exper^2),data=wage)
summary(bhat)
```
\normalsize

## Squared/Quadratic Terms in R: `hprice2` 
\scriptsize
```{r,echo=FALSE}
bhat = lm(log(price)~log(nox)+log(dist)+rooms+
               I(rooms^2)+stratio,data=hprice2)
summary(bhat)
```
\normalsize

# Interaction Effects
## Interaction Effects: Overview
Assumptions so far:

- Change in an independent variable translates into variations of the dependent variable irrespective of the level of some other independent variable.

Interaction term: The impact of one independent variable depends on the level of another independent variable.

```{r}
wage2$pareduc= wage2$meduc+wage2$feduc
bhat = lm(log(wage)~educ+educ:pareduc+exper+tenure,
          data=wage2)
```

## Interaction Effects in R
\scriptsize
```{r,echo=FALSE}
summary(bhat)
```