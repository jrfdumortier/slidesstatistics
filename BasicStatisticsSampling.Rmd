---
title: "Basic Statistics and Sampling"
author: "Jerome Dumortier"
date: "`r format(Sys.time(),'%d %B %Y')`"
output: 
     beamer_presentation:
          theme: "Hannover"
          slide_level: 3
section-titles: false
classoption: "aspectratio=169"
linkcolor: MidnightBlue
---

```{r,echo=FALSE,warning=FALSE,results=FALSE,message=FALSE,error=FALSE}
load("D:/Teaching/Data Analysis for Public Affairs/GitHub/DataAnalysisPAData.RData")
library(ggpubr)
library(ggsci)
```

## Lecture Overview
Topics covered

- Sampling
- Law of large numbers
- Central limit theorem

# Sampling

## Refresher: Population versus Sample
Population

- Entire group of individuals or items about which information is needed
- Characterized by unknown parameters

Sample 

- Subset of the population
- Application of statistics allows inference on population characteristics
- Used in (social) science to collect data without surveying entire population
- Example from natural science: Sampling of a field for soil characteristics 

Importance of sampling

- Reduction in time and resources needed to infer population characteristics
- Ability of decision-making based on sample data

Sample needs to be correctly taken which is the subject of research method classes

## Sampling Methods
Probability sampling

- Simple random sampling: Equal chance of each population member to be selected
- Stratified sampling: Separation of population into subgroups with subsequent sampling from each subgroup (e.g., based on location)
- Cluster sampling: Separation of population into clusters with subsequent selection of cluster (e.g., [fisheries]( https://doi.org/10.1080/00028487.2014.901252))

Non-probability sampling (Usually biased but useful for exploratory research)

- Convenience sampling: Participants are selected based on availability
- Quota sampling: Ensures specific characteristics are represented

# Comparison of Sampling Methods
## Exempt Organizations: Setup
Focus on the mean income of nonprofit organizations

```{r}
eo        = exemptorgs[c("name","income","ntee")]
eo        = na.omit(eo)
n         = 50 # Sample size
meaninc   = mean(eo$income)
```

Mean (population) income `r sprintf("$%s",format(meaninc,big.mark=",",nsmall=0))`

## Simple Random Sampling
Each individual has an equal chance of being selected

```{r}
simplerandom   = sample(eo$income,n,replace=FALSE)
meaninc        = mean(simplerandom)
```

Mean sample income `r sprintf("$%s",format(meaninc,big.mark=",",nsmall=0))`

## Systematic Sampling
Systematic sampling involves selecting every nth individual from a list after a random start point

```{r}
# Define a population and sample size
interval            = round(length(eo$income)/n)
start               = sample(1:interval,1)
systematicsample    = eo$income[seq(start,length(eo$income),
                                    by=interval)]
meaninc             = mean(systematicsample)
```

Mean sample income `r sprintf("$%s",format(meaninc,big.mark=",",nsmall=0))`

## Stratified Sampling by NTEE
```{r}
nteecodes = data.frame(code=c("A","B","N"),
                       description=c("Arts, Culture, and Humanities",
                                     "Education",
                                     "Recreation and Sports"))
for(i in 1:nrow(nteecodes)){
     df                  = subset(eo,ntee==nteecodes$code[i])
     df                  = sample(df$income,n,replace=FALSE)
     nteecodes$mean[i]   = sprintf("$%s",format(mean(df),
                                                big.mark=",",
                                                nsmall=0))}
```

## Convenience Sampling
Participants are chosen based on availability. In this example, we select the first 50 individuals from the data frame.

```{r}
# Take the first 50 individuals as a convenience sample
conveniencesample   = head(eo$income,50)
meaninc             = mean(conveniencesample)
```

Mean sample income `r sprintf("$%s",format(meaninc,big.mark=",",nsmall=0))`

# More on Sampling
## Problems with Sampling
Sampling Bias

- Selection bias: When sample selection leads to overrepresentation of particular groups
- Non-response bias: Results when certain respondents do not participate (i.e., respondents with similar characteristics)

Sampling errors vs. non-sampling errors

- Sampling error: The difference between the sample estimate and the true population parameter
- Non-sampling error: Errors not related to the sampling process such as measurement errors

## Sample Size and Representativeness
Importance of sample size

- Larger samples provide more precise estimates
- Reduction in the so-called margin of error

Representativeness of a sample

- Stratified sampling often ensures diverse representation
- Randomization helps avoid selection bias
- Example: Representative sample of registered voters

## Basic Calculations in Sampling
Point Estimates

- Sample mean, median, and proportions are common estimates used in public policy analysis
- Allows approximation of population parameters from sample data

Confidence Intervals

- Confidence interval (CI): Range within which a population parameter is estimated to be in

# Law of Large Numbers
## Overview
Measuring unemployment rate in the United States:

- Current Population Survey (CPS)
- Monthly survey among 60,000 households
- Classification: *Employed*, *Unemployed*, *Not in the labor force*

Law of large numbers:

- Any feature of a distribution can be recovered from repeated sampling.

Example of flipping a coin:

- Two possible outcomes: Heads or tails
- Key condition: Independence
- Expected value of heads (or tails): $E(H)=E(T)=0.5$

Difficulty to predict the share of heads from a single coin flip but high prediction precision from several thousand flips.

## Law of Large Numbers: Flipping a Coin
```{r, echo=FALSE,warning=FALSE,fig.width=5,fig.height=3}
library(ggplot2)
n   = 1000
x   = sample(0:1,n,repl=T)
s   = cumsum(x)
r   = s/(1:n)
lln = data.frame(trials=c(1:1000),heads=r)
ggplot(lln,aes(x=trials,y=heads))+geom_line()+theme_bw()+ylab("Share of Heads")+ylim(0,1)+xlab("Trials")
rm(n,x,s,r,lln)
```

## Sample versus Population
Why sampling is necessary:

- Sampling the entire population may be expensive or impossible.
- Sampling the entire population may be destructive (e.g., sampling all tires).

Random sample:

- Every item or person in the population (more specifically sample frame) has the same probability of getting selected into the sample.

Example for polling before an election:

- Every person with voting rights is in the sample frame and has the same chance of getting selected by a news agency for polling.

## Sample Mean and the Sample Variance
Estimation of the population mean based on a sample:
$$\bar{x} = \frac{1}{N} \sum^{N}_{i=1} x_i$$
Estimation of the population variance based on a sample:
$$s^2 = \frac{1}{N-1} \sum^{N}_{i=1} (x_i-\bar{x})^2$$
And this is important:

- In R, `var()` and `sd()` calculate the variance assuming a sample, i.e., division by $N-1$.

## Illustration: Estimating the Population Variance I
What we know about the population:

- Population size: 100,000
- Mean: $\mu=50$
- Standard deviation: $\sigma=20$

Sampling:

- Sample size ranging from 2 to 50
- Repeating the sampling 1000 times

## Illustration: Estimating the Population Variance II
```{r,echo=FALSE,fig.width=5,fig.height=3}
pop       = rnorm(100000,mean=50,sd=20)
n_nminus1 = matrix(0,1000,2)
out       = matrix(0,50,2)
for (j in 2:50){
     for (i in 1:1000){
          temp            = sample(pop,j)
          n_nminus1[i,2]  = var(temp)
          n_nminus1[i,1]  = var(temp)/j*(j-1)}
     out[j,1] = mean(n_nminus1[,1])
     out[j,2] = mean(n_nminus1[,2])}
out                 = sqrt(out[2:50,])
out1                = data.frame(Sample=c(2:50),Estimate=out[,1],Method="Dividing by N")
out2                = data.frame(Sample=c(2:50),Estimate=out[,2],Method="Dividing by N-1")
out                 = rbind(out1,out2)
ggplot(out,aes(x=Sample,y=Estimate,color=Method))+geom_line(aes(color=Method))+theme_bw()+
     ylim(0,25)+theme(legend.position="bottom")
rm(n_nminus1,out,out1,out2,i,j,pop,temp)
```

# Central Limit Theorem
## Sampling Distribution and Central Limit Theorem
A statistic is a random variable (with its own probability distribution) based on a sample. For example, repeated polling of 1,000 people about their political preferences will result in a different outcome each time. For the sampling distribution of the mean $\bar{x}$, we have the following:

- Mean of the sampling distribution: $\mu_{\bar{X}}$
- Variance of the sampling distribution: $\sigma^2_{\bar{X}}$
- Standard deviation of the sampling distribution (commonly known as standard error): $\sigma_{\bar{X}}$

Central Limit Theorem

- Independent of the underlying distribution, as the sample size increases, the sampling distribution of the mean will follow a normal distribution.

## Central Limit Theorem: Illustration
```{r,echo=FALSE,fig.width=5,fig.height=3}
pop_n               = 10000
sample_n            = 500
pop_uniform         = runif(pop_n,min=0,max=1)
pop_poisson         = rpois(pop_n,lambda=5)
pop_exponential     = rexp(pop_n,rate=1.5)
pop_beta            = rbeta(pop_n,0.5,0.5)
out                 = matrix(0,1000,4);
for (i in 1:1000){
     out[i,1]       = mean(sample(pop_uniform,sample_n))
     out[i,2]       = mean(sample(pop_poisson,sample_n))
     out[i,3]       = mean(sample(pop_exponential,sample_n))
     out[i,4]       = mean(sample(pop_beta,sample_n))}
  par(mfrow=c(2,4))
    hist(pop_uniform,main="Population: Uniform",xlab="")
    hist(out[,1],main="Sample: Uniform",xlab="")
    hist(pop_poisson,main="Population: Poisson",xlab="")
    hist(out[,2],main="sample: Poisson",xlab="")
    hist(pop_exponential,main="Population: Exponential",xlab="")
    hist(out[,3],main="Sample: Exponential",xlab="")
    hist(pop_exponential,main="Population: Beta",xlab="")
    hist(out[,4],main="Sample: Beta",xlab="")
```

## Central Limit Theorem: Implications for Estimation
The standard error of the mean is given by:
$$\sigma_{\bar{x}} = \sqrt{\frac{\sigma^2}{n}} =  \frac{\sigma}{\sqrt{n}}$$
The sample standard deviation is the statistic defined by:
$$s=\sqrt{s^2}$$
Suppose you have to predict the share of heads after flipping a coin multiple times. The variance of $n$ coin flips is:
$$\text{Var}(n) = \frac{p \cdot (1-p)}{n}$$

Hence: $\text{Var}(1) = 0.5$, $\text{Var}(10) = 0.025$, $\text{Var}(1000) = 0.00025$, etc.

## Application: Insurance Market
Risk aversion for individuals as well as for firms.

- Why do insurance companies exist?

Example:
$$\text{Pr}(fire) = 1/250$$
Simulation

1. Simulate the damage of $n$ homeowners
2. Calculate the share
3. Repeat 1,000 times
4. Generate histogram

## Insurance Market

```{r,echo=FALSE,fig.width=5,fig.height=3}
ndraws         = 1000;
fireinsurance  = function(ninsur){
     fire = matrix(0,ndraws,1)
          for (i in 1:ndraws){
               fg      <- sample(x=c(1,0),size=ninsur,replace=TRUE,prob=c(1/250,249/250))
               fg      <- sum(fg)/ninsur
               fire[i] <- fg}
               return(fire)}
fire1     = data.frame(value=fireinsurance(1000),item="1000 People Insured")
fire2     = data.frame(value=fireinsurance(10000),item="10000 People Insured")
fire3     = data.frame(value=fireinsurance(25000),item="25,000 People Insured")
fire4     = data.frame(value=fireinsurance(100000),item="100,000 People Insured")
fire      = rbind(fire1,fire2,fire3,fire4)
ggdensity(fire,x="value",color="item",fill="item",palette="jco")
```

## Sampling Variance
Sampling Variance is the variability in a sample statistic (e.g., sample mean) across different samples drawn from the same population.

- Reflection of the spread of sample estimates around the population parameter
- Lower variance indicates a more stable estimate, while higher variance suggests more fluctuation between samples.
- Key in determining the margin of error and confidence intervals

## Factors Influencing Sampling Variance
Sample Size and Sampling Variance

- **Larger samples** generally have lower sampling variance, resulting in more precise estimates.
- **Smaller samples** tend to have higher variance, making estimates less reliable.

Population Variability
- If the population itself has high variability, samples will also exhibit higher sampling variance.
- Lower population variability translates to lower sampling variance, even with smaller samples.

A larger sample size reduces sampling variance, leading to smaller confidence intervals
This reduction follows the Law of Large Numbers, which states that as sample size increases, the sample mean approaches the population mean.

